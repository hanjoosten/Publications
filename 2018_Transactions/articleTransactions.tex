\documentclass{elsarticle}
\usepackage{graphicx}
%\usepackage{multicol}
%\usepackage{footmisc}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
%\usepackage[official,right]{eurosym}
\selectlanguage{english}
\include{preamramics}
\hyphenation{ExecEngine}
\newtheorem{lemma}{Lemma}
\def\id#1{\text{\em #1\/}}
\def\Events{{\mathit E}}
\begin{document}

\title{A Theory for Database Transactions with no Database}
\author[ou,ordina]{Stef Joosten\fnref{fn1}}
\ead{stef.joosten@ou.nl}
\author[utwente]{Sebastiaan Joosten\fnref{fn2}}
\address[ou]{Open Universiteit Nederland, Postbus 2960, 6401 DL Heerlen, the Netherlands}
\address[ordina]{Ordina NV, Nieuwegein, the Netherlands}
\address[utwente]{University of Twente, Enschede, the Netherlands}
\fntext[fn1]{ORCID 0000-0001-8308-0189}
\fntext[fn2]{ORCID 0000-0002-6590-6220}

\begin{abstract}
	How can we make transactions work in distributed information systems without a shared database?
	Transaction theory as we know it assumes a shared data space.
	Multiple processes are acting simultaneously, each making changes to that shared data space.
	The observation that transaction mechanisms in NoSQL databases are not common
	makes us feel that a solution to this problem is not so obvious.

	This contribution proposes a theory of database transactions without a shared data space.
	This makes transactions suitable for more databases than just relational ones.
	The soundness of the theory has been established by means of formal proof.
	To show that the theory works, the theory is demonstrated by a prototype developed in Ampersand\footnote{\tt https://github.com/AmpersandTarski/Ampersand}.
	To demonstrate that the theory is about real transactions, this contribution defines semantics of conventional database transactions.
	To fulfill our promise, we also gives semantics of transactions on the semantic web.
\end{abstract}

\begin{keyword}
relation algebra\sep software development\sep legal reasoning\sep information systems design\sep Ampersand\sep MirrorMe\sep reactive programming
\end{keyword}
\maketitle

\section{Introduction}
\label{sct:Introduction}
	A distributed information system (DIS) serves its users with things such as
	registering data, guarding the consistency, protecting the data,
	and providing that data accurately and meaningfully for its intended purpose.
	Users want predictable behaviour when they are simultaneously changing a shared state (e.g.\ flight reservation).
	For that purpose a transaction mechanism preserves system-wide invariants.
	Many transaction mechanisms are known that presume a shared database [refs].
	However, shared databases are not always an option in distributed cloud architectures.
	Globally distributed applications such as Netflix and LinkedIn cannot afford to rely on a shared database for obvious reasons of performance and reliability.
	Many cloud applications use distributed storage models instead.
	Examples are event stores (e.g.\ Kafka), triple stores (e.g.\ semantic web), graph databases (e.g. Neo4J) and others.
	These so-called NoSQL databases often lack functionality for transactions,
	leaving the problems for the user to solve.
	As a result, many architectural patterns exist for the purpose of doing transactions in a distributed setting.
	Some of these patterns simply circumvent the problem.
	Others will isolate transactionality into a smaller sub-problem.
	Still others introduce locking mechanisms to do the job.
	In paper we will show that the ACID-properties of transactions follow quite naturally from the theory of time.
	In informal terms: the merge of Git corresponds to the transactional commit in databases.
	This result contributes to the body of foundational theory of information systems.
	
	This paper describes transactions without the assumption of a shared database.
	In we express the characteristic properties of transactions: Atomicity, Consistency, Isolation, and Durability.
	{\em Atomicity} means that a sequence of primitive actions can be executed as though it were a single action.
	{\em Consistency} means that integrity rules are warranted by the database management system.
	{\em Isolation} means that actions can be done without any knowledge of what other users are doing simultaneously.
	{\em Durability} means that the contents of a database cannot change unintendedly.
	For this purpose we build on a well-known theory of time~\cite{DBLP:journals/ipl/FosterCWZ18},
	which is generally applicable,
	well-established, and founded in a solid mathematical structure~\cite{Foster14}.

\section{Related work}
\label{sct:Related work}
	In 2017, Simon Foster et al. published a theory of time with generalised reactive processes~\cite{DBLP:journals/ipl/FosterCWZ18}.
	It provides a generic foundation for denotational semantics of concurrent languages,
	the proof obligation of which are satisfied by automated verification~\cite{Foster14}.
	Their theory describes program behaviour as relational predicates whose variables correspond to observable quantities.
	It draws on the seminal work of Hoare on communicating sequential processes~\cite{HoareHe98},
	work on process algebras by Bergstra and Klop~\cite{DBLP:journals/iandc/BergstraK84}, and
	Robin Milner's work on CCS~\cite{Milner1989}.
	We chose to build our theory on the work of Foster et al. because it provides three healthiness conditions
	that theories of time must adhere to,
	ready for us to adopt.
	In the same style we have tried to keep our theory down to the bare minimum, to make it as generally applicable as possible.
	We too have used Isabelle to verify the required proofs,
	to increase our confidence in this theory.
	
\cite{Foster17c}
	In the nineties, scientists saw that category theory can be used to formalize database schemas~\cite{Frederiks1997},
	which was advocated as a means to further foundational theories of information systems.
	An interesting read is~\cite{Foster17c}\footnote{\tt\tiny https://towardsdatascience.com/distributed-transactions-and-why-you-should-care-116b6da8d72}.
	It explains how modern distributed database cope with ACID properties in a way that makes sense.

	ACID 2.0 - Associative, Commutative, Idempotent, Distributed

\section{History}
\label{sct:History}
	Distributed information systems are dynamic systems,
	so time is the first aspect we focus on.
	Being distributed means that we cannot assume that the temporal order of events is known in all cases.
	In this paper we simplify the notion of time to precedence of events,
	allowing us to study the evolution over time by events that happen.
	We need the notion of {\em event}\footnote{Cambridge Dictionary defines ``event'' as: anything that happens} to describe things that happen in a DIS.
	In this work we use variables $e$, $e_1$, and $e_2$ to represent events.
	Let $\mathbb E$ represent the set of all conceivable events.
	We also introduce the notion of {\em history structure}
	to represent all events that have happend up to some point in time and the precedences between those events.
	In this paper, variables $h$, $h'$, $h_1$, and $h_2$ represent history structures.
	Let $\mathbb H$ be the set of all conceivable history structures.
	We describe the history structure of a DIS as a partial order of events

	A {\em history structure} is a tuple $\la \Events, \id{prec}\ra$ in which
\begin{eqnarray}
	&&\id{prec}\ \subseteq\ \Events\times\Events\label{req:precInE}\\
	&&(e,e)\notin\id{prec}\label{req:prec irreflexive}\\
	&&(e_1,e_2)\in\id{prec}^+\ \wedge\ (e_2,e_1)\in\id{prec}^+\ \Rightarrow\ e_1=e_2\label{req:precPlus asy}
\end{eqnarray}
	In a history structure $\la \Events, \id{prec}\ra$ set $\Events$ represents the events that have been registered up to some point in time.
	For every event $e\in\Events$ we can say that it has happened (at some time in the past).
	Let $e_1\ \id{prec}\ e_2$ represent the fact that event $e_1$ has happened before event $e_2$,
	making $\id{prec}$ a relation on events in $\Events$.
	Requirement~\ref{req:prec irreflexive} excludes the situation that $e$ has happened before itself.
	To represent time faithfully, we require that $\la\Events,\id{prec}^*\ra$ is a partially ordered set,
	meaning that $\id{prec}^*$ is reflexive, transitive, and antisymmetric.
	For this reason equation~\ref{req:precPlus asy} requires the transitive closure $\id{prec}^+$ to be antisymmetric.

	The special case in which $\la\Events,\id{prec}^*\ra$ is a total order is called a {\em sequential history structure}.
	Note that any nonempty finite sequential history structure has one current event and one first event.
	Other finite history structures may have more.
	In a sequential history structure, every event except the first one has precisely one preceding event.
	In general, however, events may have multiple preceding events and multiple succeeding events.

	In a history structure $\la \Events, \id{prec}\ra$, an event $e$ that precedes another event is called a {\em past} event,
	which we define as \ $\exists e_1\in\Events: e\ \id{prec}\ e_1$.
	All other events are called {\em current} events in that history structure.
	Events in $\Events$ that have no preceding events are called {\em first} events.

	We extend the notion of subset to historic structures and use the subset symbol $\subseteq$ for that purpose.
	We call this a {\em historic substructure}, which is defined by:
\begin{equation}
\label{def:historic substructure}
	\la \Events, \id{prec}\ra\ \subseteq\ \la \Events', \id{prec}'\ra\ \Leftrightarrow\ \Events\subseteq\Events'\ \wedge\ \id{prec}\subseteq\id{prec}'
\end{equation}

	A DIS grows by adding events to it.
	We describe each time a new event $e$ happens as a mapping, $\id{happens}_{P,e}$, from one history structure to another:
\begin{equation}
\begin{array}{l}
	P\subseteq\Events\ \wedge\ e\notin E\\
	\ \ \Rightarrow\ \ \id{happens}_{P,e}(\la \Events, \id{prec}\ra)\ =\ \la \Events\cup\{e\}, \id{prec}\cup\{(e_1,e) | e_1\in P\}\ra
\end{array}
\label{req:def happens}
\end{equation}
	This happening adds one event to $\Events$ and enlarges relation $\id{prec}$ with one pair for every immediate predecessor of $e$.
	The immediate predecessors of $e$ are specified in set $P$, which we call the predecessor set.
	We need the predecessor set to cater for the distributed nature of a DIS.
\begin{lemma}
\label{lemma:happens preserves history structure}
	If $\la \Events, \id{prec}\ra$ is a history structure and $P\subseteq\Events$ and $e\notin\Events$
	then $\id{happens}_{P,e}(\la \Events, \id{prec}\ra)$ is a history structure.
\end{lemma}
	Lemma~\ref{lemma:happens preserves history structure} is a preservation property.
	It says that $\id{happens}_{P,e}$ preserves the history structure,
	provided that $P\subseteq\Events$ and $e\notin\Events$.
	This lemma has been proven in proof assistant Isabelle/HOL.
	To understand why $P\subseteq\Events$ is necessary,
	let $\la \Events', \id{prec}'\ra=\id{happens}_{P,e}(h)$.
	Now suppose event $e_1\in P$ but $e_1\notin\Events$.
	Equation~\ref{req:def happens} tells us that $(e_1,e)\in\id{prec}'$.
	However, this violates requirement~\ref{req:precInE}.
	So it is necessary that every $e_1\in P$ is an element of $\Events$.
	Similarly, $e\notin\Events$ is a necessary condition,
	because any $e\in\Events$ violates the antisymmetry of $\id{prec}^+$ (requirement~\ref{req:precPlus asy}).

	With lemma~\ref{lemma:happens preserves history structure} we can prove that $\id{happens}_{P,e}(h)=h'$ implies that $h\subseteq h'$.

	Let us define predicate $\id{HS}\la \Events, \id{prec}\ra$
	to denote that $\la \Events, \id{prec}\ra$ is a history structure.
\begin{eqnarray}
	&&\id{HS}\la \emptyset, \emptyset\ra\label{def:zeroHS}\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{HS}\la \Events, \id{prec}\ra\ \Rightarrow\ \id{HS}(\id{happens}_{P,e}\la \Events, \id{prec}\ra)
\end{array}
\label{def:HS}
\end{eqnarray}
	Equation~\ref{def:zeroHS} says that $\la \emptyset, \emptyset\ra$ is a history structure.
	Equation~\ref{def:HS} specifies recursively that $\id{happens}_{P,e}(h)$ is a history structure iff
	$h$ is a history structure and if $P$ and $e$ satisfy the specified conditions.
	This definition allows us to prove properties of finite history structures by induction.
	In practical situations, every history structure is finite.

	Note that $\id{happens}_{P,e}$ can make current events become past events.
	If $h'=\id{happens}_{P,e}(h)$, every event in $P$ is a past event in $h'$ and event $e$ becomes current in $h'$.

\section{States}
	This section defines the requirements for predictable behavior of a distributed information system (DIS).
	We consider behavior to be predictable if every state can be computed unambiguously from its history.
	For that purpose we introduce the notion of state.
	Let $\mathbb S$ represent the set of all conceivable states.

	This section expands history structures to stateful history substructures
	to represent the (global) state of an information system at any point in time.
	Since this paper is about distributed systems,
	we may not assume that the global state of a DIS at some point in time is observable.
	The state of a DIS is observable only in its local parts.
	So we shall define multiple states, each of which may be interpreted locally.

	We define a {\em stateful history structure} as a triple $\la\Events, \id{prec}, \id{state}\ra$ in which
	$\la \Events, \id{prec}\ra$ is a history structure and $\id{state}:\Events\rightarrow{\mathbb S}$ is a function
	that maps every event in $\Events$ to a state.
	We will use predicate $\id{SHS}\la \Events, \id{prec}, \id{state}\ra$
	to express that $\la \Events, \id{prec}, \id{state}\ra$ is a stateful history structure.
	Let $s=\id{state}(e)$ represent the fact that $s$ is the state immediately after event $e$ has happened.
	Or shorter: $s$ is the state after event $e$.
%	Instead of $s=\id{state}(e)$, we may also write $(s,e)\in\id{state}$ because a function is a univalent and total relation
%	(and therefore a set of pairs).
	A {\em current state} is the state after a current event
	and a {\em past state} is the state after a past event.

	In a stateful history structure, the occurrence of an event $e$ defines a new history structure.
	This is the way systems evolve over time.
	So we define the occurrence of an event as a mapping, $\id{occurs}_{P,e,s}$, from one SHS to another SHS:
\begin{equation}
\begin{array}{l}
	P\subseteq\Events\ \wedge\ e\notin E\\
	\ \ \Rightarrow\ \ 
\begin{array}[t]{l}
	\id{occurs}_{P,e,s}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l@{\hspace{.2em}}c@{\hspace{.2em}}ll}
		\text{Let }\id{newState}(x)&=&s\text{,}&\text{if }x=e\\
				&=&\id{state}(x)\text{,}&\text{otherwise}\\
		\multicolumn{4}{@{}l}{\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \id{newState}\}\ra}
		\end{array}
\end{array}
\end{array}
\label{req:def occurs}
\end{equation}
	This definition extends the definition of $\id{happens}$ (eqn.~\ref{req:def happens}).
	By using lemma~\ref{lemma:happens preserves history structure} we can prove that $\id{occurs}_{P,e,s}$ preserves
	the stateful history structure.
	Also, we will use the subset symbol $\subseteq$ to represent the fact that $h$ is a {\em stateful historic substructure} of $h'$.
	And it will come as no surprise that $\id{occurs}_{P,e,s}(h)=h'$ implies that $h\subseteq h'$.
	We will use predicate $\id{SHS}\la \Events, \id{prec}, \id{state}\ra$
	to express that $\la \Events, \id{prec}, \id{state}\ra$ is a stateful history structure.
	To describe the growth of a stateful history structure by the event,
	we define $\id{SHS}$ recursively using $\id{occurs}_{P,e,s}$ as recurring step:
\begin{eqnarray}
	&&\id{SHS}\la \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{SHS}\la \Events, \id{prec}, \id{state}\ra\\
	\ \ \Rightarrow\ \id{SHS}(\id{occurs}_{P,e,s}\la \Events, \id{prec}, \id{state}\ra)
\end{array}
\label{req:recursive def occurs}
\end{eqnarray}

\section{Transactions}
\label{sct:Transactions}
	In practice, a new state is computed from the current state by specifying a change, $\Delta$.
	Such changes are called {\em transactions}.
	To describe a DIS in terms of changes, we need operators to compute a new state ($s_2$) from a given state ($s_1$).
	In fact, we need two operators, $\oplus$ and $\ominus$, such that
\begin{equation}
	s_2\ =\ s_1\oplus(s_2\ominus s_1)
\label{req:oplus and ominus}
\end{equation}
	Let us call $s_2\ominus s_1$ the difference ($\Delta$) between states $s_2$ and $s_1$.
	We use $s_1\oplus\Delta$ to compute a new state from an existing state $s_1$ and a change $\Delta$.

	We can define a stateful history structure,
	in which the state after every event is computed from a change, $\Delta$.
	For this purpose we must designate one specific state, $\mathcal I\in\mathbb S$, as the initial state.
	We must additionally specify an existing event $e_1$ in order to compute $\id{state}(e_1)\oplus\Delta$.
	The following mapping $\id{occurByDelta}_{P,e_1,\Delta,e}$ defines how an SHS grows by one event ($e$)
	using only the change between the state of event $e_1$ to the new event $e$:
\begin{eqnarray}
&&\begin{array}{l}
	\id{occurByDelta}_{P,e_1,\Delta,e}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l@{\hspace{.2em}}c@{\hspace{.2em}}ll}
		\text{Let }s_1&=&\id{state}(e_1)\text{,}&\text{if }e_1\in P\\
				&=&\mathcal I\text{,}&\text{otherwise}\\
		\text{Let }\id{newState}(x)&=&s_1\text{,}&\text{if }x=e\\
		&=&\id{state}(x)\text{,}&\text{otherwise}\\
		\multicolumn{4}{@{}l}{\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \id{newState}\}\ra}
		\end{array}
\end{array}
\label{req:SHS by Delta}
\end{eqnarray}
	This mapping serves as recursion step in the following definition of a stateful history structure
\begin{eqnarray}
	&&\id{SHS}\la \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{SHS}\la \Events, \id{prec}, \id{state}\ra\\
	\ \ \Rightarrow\ \id{SHS}(\id{occurByDelta}_{P,e_1,\Delta,e}\la \Events, \id{prec}, \id{state}\ra)
\end{array}
\label{req:recursive def SHS by Delta}
\end{eqnarray}
	In practice, information systems build states incrementally.
	Each event comes with an increment ($\Delta$) that changes the current state.
	For this purpose we define an incremental history structure (IHS).
	A {\em incremental history structure} is a quadruple $\la\Events, \id{prec}, \delta, \iota\ra$ in which:
\begin{itemize}
\item	$\la \Events, \id{prec}\ra$ is a history structure;
\item	$\delta(e_1,e_2)$ represents a change, $\id{state}(e_2)\ominus\id{state}(e_1)$, for every pair $(e_1,e_2)\in\id{prec}$;
\item	$\iota(e)$ represents the change $\id{state}(e_1)\ominus\mathcal I$ for every event $e\in\Events$ that has no predecessor in $\id{prec}$.
\end{itemize}
	Since histories are built event-by-event, we define IHSs incrementally.
	For that purpose we use the following mapping, $\id{increment}_{P,e_1,e,\Delta,\iota}$,
	which adds one event ($e$) to an IHS:
\begin{eqnarray}
\begin{array}{l@{}l}
	\multicolumn{2}{l}{\id{increment}_{P,e_1,e,\Delta}\la\Events, \id{prec}, \delta, \iota\ra}\\
	\ \ =\ &\begin{array}[t]{@{}l@{\hspace{0.2cm}}c@{\hspace{0.2cm}}ll}
		\text{Let }\delta'(x,y)&=&\Delta\text{, }&\text{if }x=e_1\wedge y=e\wedge x\in P\\
				       &=&(\id{state}(e_1)\oplus\Delta)\ominus\id{state}(x)\text{, }&\text{if }x\neq e_1\wedge y=e\wedge x\in P\\
				       &=&\delta(x,y)\text{, }&\text{if }(x,y)\in\id{prec}
		\end{array}\\
		&\begin{array}[t]{@{}l@{\hspace{0.2cm}}c@{\hspace{0.2cm}}ll}
			\text{Let }\iota'(x)&=&\Delta\text{, }&\text{if }x=e\wedge P=\emptyset\\
				    &=&\iota(x)\text{, }&\text{if }x\neq e\wedge P=\emptyset
		\end{array}\\
		&\text{in }\la\Events\cup\{e\}, \id{prec}\cup\{(e_2,e)|e_2\in P\}, \delta', \iota'\}\ra
\end{array}
\label{req:increment}
\end{eqnarray}
	This definition inserts a new event, $e$, into a history structure with $e_1$ as its predecessor.
	The new state, $\id{state}(e)$, is computed by $\id{state}(e_1)\oplus\Delta$; let us call it $s$.
	Beside $e_1$, there may be other events preceding $e$ in the predecessor set $P$.
	The change from every predecessor $x$ can be computed by $s\ominus\id{state}(x)$.
	All these changes are added to the function $\delta$.
	The function $\iota$ is there for initialization purposes.
	To get an initial history structure (with no events in it) going,
	we must add events while there are no other events available as predecessor.
	We use the empty predecessor set as condition to signal that there is no previous state.
	The mapping $\id{increment}_{P,e_1,e,\Delta}$ can be used as recursive step in defining predicate $\id{IHS}$,
	which characterizes an incremental history structure:
\begin{eqnarray}
	&&\id{IHS}\la \emptyset, \emptyset, \emptyset, \emptyset\ra\\
&&\begin{array}{@{}l}
	P\subseteq\Events\ \wedge\ e\notin E\ \wedge\ \id{IHS}\la \Events, \id{prec}, \delta, \iota\ra\\
	\ \ \Rightarrow\ \id{IHS}(\id{increment}_{P,e_1,e,\Delta}\la \Events, \id{prec}, \delta, \iota\ra)
\end{array}
\label{req:recursive def IHS}
\end{eqnarray}
The following mapping transforms a stateful history structure to an incremental history structure:
\begin{eqnarray}
&&\begin{array}{l}
	\id{shs2ihs}(\la\Events, \id{prec}, \id{state}\ra)\\
	\ \ =\ \begin{array}[t]{@{}l}
		\text{Let }\forall(e_1,e_2)\in\id{prec}:\ \delta(e_1,e_2)=\id{state}(e_2)\ominus\id{state}(e_1)\\
		\text{Let }\id{initial}=\{e\in\Events|\forall e_1\in\Events: (e_1,e)\notin\id{prec}\}\\
		\text{Let }\forall e\in\id{initial}:\ \iota(e)=\id{state}(e)\ominus\mathcal I\\
		\text{in }\la\Events, \id{prec}, \delta, \iota\ra
		\end{array}
\end{array}
\label{req:shs2ihs}
\end{eqnarray}
	Vice versa, we can also map an incremental history structure to a stateful history structure:
\begin{eqnarray}
&&\begin{array}{l}
	\id{ihs2shs}(\la\Events, \id{prec}, \delta, \iota\ra)\\
	\ \ =\ \begin{array}[t]{@{}l}
		\text{Let }\forall(e_1,e_2)\in\id{prec}:\ \id{state}(e_2)=\id{state}(e_1)\oplus\delta(e_1,e_2)\\
		\text{Let }\id{initial}=\{e\in\Events|\forall e_1\in\Events: (e_1,e)\notin\id{prec}\}\\
		\text{Let }\forall e\in\id{initial}:\ \id{state}(e)=\mathcal I\oplus\iota(e)\\
		\text{in }\la\Events, \id{prec}, \id{state}\ra
		\end{array}
\end{array}
\label{req:ihs2shs}
\end{eqnarray}
	The\marginpar{claim} fact that $\id{shs2ihs}$ and $\id{ihs2shs}$ are each other's inverse proves that the choice to represent
	history by an IHS or SHS can be made on practical grounds.

	We must prove\marginpar{claim} that
\begin{equation}
	\id{ihs2shs}(\id{increment}_{P,e_1,e,\Delta}(\id{shs2ihs}(h)))\ =\ \id{occurs}_{P,e,\id{state}(e_1)\oplus\Delta}(h)
\end{equation}

\section{Isolation}
\label{sct:Isolation}
	How does a user perceive a distributed information system (DIS)?
	There are different options.
	An example is an information screen with flight arrival times in an airport,
	where a user sees the current state of flight arrivals and sees it change each time a new event occurs.
	This is an example of non-transactional use: the user consumes information passively.
	However, the topic of this paper is transactional use,
	an example of which is to make a flight reservation.
	In that example, the user interacts with the system for the purpose of reserving a seat on a flight.
	She sees a current state and provides events to the system by filling out fields in a reservation screen.
	These events cause changes in the state (e.g.\ from {\em The flight is not paid} to {\em The flight has been paid}).
	With this type of use, i.e transactional use, the user expects a predictable result.
	That is: if the current state is $s$ and the user specifies a change $\Delta$,
	the resulting state must be $s\oplus\Delta$.
	This requirement is the incremental variant of requirement~\ref{req:transaction}:
\begin{equation}
	h'=\id{occurByDelta}_{P,e_1,\Delta,e}(h)\ \Rightarrow\ \id{state}(e)\ =\ \id{state}(e_1)\oplus\Delta
\label{req:occurByDelta}
\end{equation}
	Since $\id{occurByDelta}$ preserves a stateful history structure (equation~\ref{req:recursive def SHS by Delta}),
	the user can interact repeatedly with the system and update the state predictably in a sequence of changes.
	This yields a user experience of being the only user of the system.
	That property is called {\em isolation}.

\section{Consistency}
\label{sct:Consistency}
	From a user perspective, {\em consistency} means that the system is free of contradictions.%
\footnote{A user might have wanted the system to provide the truth itself,
	but that is beyond what technical systems can guarantee. For people can feed the system with lies.
	However, freedom of contradictions is quite desirable as second-best, because truth is free of contradictions.}
	We require a predicate on states to determine whether a state is consistent.
	So we extend the notion of stateful historic structure with that predicate.

	A {\em nonconflicting history structure} is a quadruple $\la\Events, \id{prec}, \id{state}, \id{consistent}\ra$ in which
	$\la \Events, \id{prec}, \id{state}\ra$ is a SHS and $\id{consistent}$ is a predicate on $\mathbb S$,
	which holds for every state in the SHS.
	We will use predicate $\id{NHS}$ as a definition:
\begin{equation}
\begin{array}{l@{}ll}
	\multicolumn{3}{l}{\id{NHS}\la \Events, \id{prec}, \id{state}, \id{consistent}\ra}\\
	\ \Leftrightarrow\ &\id{SHS}\la \Events, \id{prec}, \id{state}\ra&\wedge\\
	&\id{consistent}\subseteq\mathbb S&\wedge\\
	&\forall e\in\Events:\ \id{consistent}(\id{state}(e))&
\end{array}
\label{def:NHS}
\end{equation}
	Equation~\ref{def:NHS} defines a nonconflicting history structure:
	For practical use, we require $\id{consistent}$ to be decidable.

\section{Atomicity}
\label{sct:Atomicity}
	{\em Atomicity} guarantees all-or-nothing outcomes.
	Let an example clarify this.
	If I purchase a song on the internet, I must pay.
	As a result, it is acceptable that the payment is done and the song is delivered.
	It is also acceptable that the payment is not done and the song is not delivered.
	However, it is unacceptable that the payment is done and the song is not delivered or vice-versa.

	Consider this:
	\[h'\ =\ \id{increment}_{P,e_2,e_3,\Delta_2}(\id{increment}_{P,e_1,e_2,\Delta_1}(h))\]
	Imagine that $\Delta_1$ represents my payment and $\Delta_2$ represents the delivery of my song.
	In the corresponding history structure, we will see that
	\[\{(e_1,e_2), (e_2,e_3)\}\subseteq\id{prec} \]
	It means that the overall state change caused by a sequence of events can be committed as a whole or rejected completely.

	So let us first discuss event sequences.
	Let us define an event sequence as a totally ordered history structure.
	Note that a history structure that is not totally ordered may contain multiple historic substructures
	that are totally ordered.
	Typically, event sequences occur multiply in a history structure.

\section{Durability}
\label{sct:Durability}
	In the eyes of the user, durability means that history is not changed.
	In the previous definitions durability has been secured by defining every change as an expansion of an existing system.
	Events are added only. Once added they are never removed or replaced.
	This corresponds to working practices in financial ledgers.
	Transactions are never removed, only compensated.

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}

\section{Threads}
	So how does a stateful history structure represent a distributed information system?
\marginpar{claim}
	Part of the answer is that a substructure of a stateful history structure is a stateful history structure by itself.

	The answer is that we have made no assumptions about location,
	like for instance that there is a shared memory.
	Even if we assume that each state exists in one location,
	a stateful history structure can still have multiple current states.
	Each of these states can exist in a separate location.

	The purpose of threads is to support deterministic computations.
	We consider any nonempty history structure that is totally ordered to be a thread.
	A {\em thread} is a history structure $\la \Events, \id{prec}\ra$ that satisfies
\begin{eqnarray}
	\forall e_1,e_2&:&e_1\in\Events\ \wedge\ e_2\in\Events\ \Rightarrow\ e_1\ \id{prec}+\ e_2\vee e_2\ \id{prec}^+\ e_1\label{eqn:order is total}\\
	\exists e&:&e\in\Events\label{eqn:nonempty thread}
\end{eqnarray}
	The total order (\ref{eqn:order is total}) is required to get deterministic results from a computation in a thread.
	In order to talk about the first and the last event in a thread, we require at least one event (\ref{eqn:nonempty thread}) in a thread.
	We shall use variable $t$ to represent threads.
	The first event in a thread $t$ is the infimum of the events in $t$;
	the last event is the supremum.

	We need the notion of state%
	\footnote{Cambridge Dictionary meaning of ``state'':
	a condition or way of being that exists at a particular time}
	to describe things that change over time.
	We shall use variables $s$, $s_1$, and $s_2$ to denote states.
	Let $\id{state}(e)$ represent the state after event $e$ has happened.

\section{Distributed Information System}
	
	Now equation~\ref{eqn:eval step} is the recursion step and the initial state is
	the starting point of the recursive computation of the final state of $t$.
	We consider a distributed information system as a set of processes that share a history.
	The system is distributed because processes need not share all histories.
	We need the notion of process%
	\footnote{Cambridge Dictionary meaning of ``process'':
		a series of actions that you take in order to achieve a result}
	to describe how components of a DIS interact.
	Let $\mathbb P$ be the set of all processes.
	We have formulated the following requirements:
	\begin{itemize}
	\item Belong/Contain\\
		To represent the fact that an event {\em belongs to} a process or
		(equivalently) that a process {\em contains} certain events,
		let $e\in p$ represent the statement that event $e$ belongs to process $p$.
	\item Shared history\\
		To abstract away from location, we make no assumptions with respect to the place where a process lives.
		However, to partake in a distributed system, processes must be able to share history.
		For that purpose we define what it means to be part of a history:
		\begin{equation}
			p\ \id{partof}\ h\ \Leftrightarrow\ (\forall e: e\in p\Rightarrow e\in h)
		\end{equation}
		We define an order between processes that share a history.
		\begin{equation}
			p_1\ \preceq\ p_2\ \Leftrightarrow(\forall e: e\in p_1\Rightarrow e\in p_2)
		\end{equation}
		We can prove that
		\begin{equation}
			p_1\ \preceq\ p_2\ \Rightarrow\ (\exists h:p_1\ \id{partof}\ h\wedge p_2\ \id{partof}\ h)
		\end{equation}
	\item Total order\\
		We require a process to have predictable behaviour, for which we need the process to contain a series (sequence) of events.
		So we require that $(\{e|e\in p\}, \preceq)$ is a total order.
		Apart from being a partial order, $\preceq$ must satisfy (for every $e_1$, $e_2$, and $p$):
		\begin{equation}
			e_1\in p\ \wedge\ e_2\in p\ \Rightarrow\ e_1\preceq e_2\vee e_2\preceq e_1
		\end{equation}
	\end{itemize}
	
\section{States}
	\begin{itemize}
	\item Revert\\
		A change can be used to compute a previous state.
		For this purpose we require an operator $\ominus$ such that
	\begin{equation}
		s_1\oplus\Delta=s_2\ \Leftrightarrow\ s_1=s_2\ominus\Delta
	\end{equation}
		As a consequence, we have for every event $e$:
	\begin{equation}
		\id{state}(\id{past}(e))=\id{state}(\id{pres}(e))\ominus\id{delta}(e)
	\end{equation}
		With $\preceq$ being reflexive, antisymmetric, and transitive,
		it follows immediately that $\oplus$ is idempotent, commutative, and associative.
	\end{itemize}



\section{Isolation}
\label{sct:Isolation}
{\em Isolation} means that actions can be done without any knowledge of what other users are doing simultaneously.

\section{Durability}
\label{sct:Durability}
{\em Durability} means that the contents of a database cannot change unintendedly.

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}

%% Obsolete

\section{Consistency}
\label{sct:Consistency}
Each user of a database issues a stream of events.
	The order (in time) of events is given by a relation\footnote{%
A relation $\declare{r}{A}{B}$ is a set of pairs. Each pair $\pair{a}{b}$ in that set satisfies $a\in A$ and $b\in B$}
	$\declare{pred}{Event}{Event}$,
	where $\pair{e1}{e2}\in\id{pred}$ means that event $e1$ precedes event $e2$.
	From a user perspective this stream is a sequence, so every event except the first has exactly one predecessor.
	
	For simplicity's sake, we assume the user sees a finite set of items at any given moment.
	The items seen by the user at any moment are given by a relation $\declare{state}{Event}{Item}$,
	where $\pair{e}{i}\in\id{state}$ means that item $i$ exists immediately after $e$ has happened.
	So the set of items visible to the user immediately after $e$ has happened can be computed by $\{ i | \pair{e}{i}\in\id{state} \}$.
	Note that the relation $\id{state}$ is interpreted in an open world.
	If $\pair{e}{i}\not\in\id{state}$ this does not mean $i$ does not exist.
	It just means that the pair $\pair{e}{i}$ is not in $\id{state}$, so the user will not see it.

	To cope with change, a user may insert and/or delete items at will.
	Let us specify these deletions and insertions by two relations, $\declare{ins}{Event}{Item}$ and $\declare{del}{Event}{Item}$,
	where $\pair{e}{i}\in\id{ins}$ means that pair $\pair{e}{i}$ is to be added to $\id{state}$,
	and $\pair{e}{i}\in\id{del}$ means that pair $\pair{e}{i}$ is to be removed from $\id{state}$.
	More precisely, let us consider an event $e$.
	The items $\{ i | \pair{e}{i}\in\id{state} \}$ are the items that exist immediately after the preceding event has happened,
	minus the items to be removed, plus the items to be inserted.
	The following equation defines this \define{behaviour}:
\begin{equation}
	\id{state}\ =\ (\id{pred};\id{state}-\id{del})\cup\id{ins}
\label{behaviour}
\end{equation}
	Equation~\ref{behaviour} says that the items visible to the user immediately after an event $e$
	are all items that are inserted together with items that were visible immediately after the preceding event, except those that were deleted.

	We define $A_{(\id{ins},\id{del})}$ as the function\footnote{The name $A$ is chosen as a reminder to the word ``action''.}
	that embodies this change:
\begin{equation}
	A_{(\id{ins},\id{del})}(x)\ =\ (x-\id{del})\cup\id{ins}
\end{equation}

	This raises the question whether we can compose two actions into a single action.
	For this, we derive:
\[\begin{array}{r@{~}l}
	&(((x-\id{del})\cup\id{ins})-\id{del}')\cup\id{ins}'\\
	=&\hspace{1in}\{x-y=x\cap\overline{y}\}\\
	&(((x\cap\overline{\id{del}})\cup\id{ins})\cap\overline{\id{del}'})\cup\id{ins}'\\
	=&\hspace{1in}\{\text{distribute $\cap$ over $\cup$}\}\\
	&(x\cap\overline{\id{del}}\cap\overline{\id{del}'})\cup(\id{ins}\cap\overline{\id{del}'})\cup\id{ins}'\\
	=&\hspace{1in}\{\text{De Morgan}\}\\
	&(x-(\id{del}\cup\id{del}'))\cup(\id{ins}-\id{del}')\cup\id{ins}'\\
	=&\hspace{1in}\{\text{Assume}\ \id{ins}\cap\id{del}=\emptyset\ \text{and}\ \id{ins}'\cap\id{del}'=\emptyset\}\\
	&(x-(\id{del}\cup\id{del}'))\cup(\id{ins}\cup\id{ins}')
\end{array}\]
	This yields composition ($;$) of $A$:
\begin{equation}
	A_{(\id{ins},\id{del})}\ ;\ A_{(\id{ins}',\id{del}')}\ =\ A_{((\id{ins}-\id{del}')\cup\id{ins}', \id{del}\cup\id{del}')}
\label{composition of A}
\end{equation}
	
	The composition operator turns out to be commutative, but under a condition.
	This will appear to be useful later on in this paper.
	Equation~\ref{composition of A} shows that we only need to investigate commutativity in the first argument of $A$: 
\[\begin{array}{r@{~}rcl}
	&A_{(\id{ins},\id{del})}\ ;\ A_{(\id{ins}',\id{del}')}&=&A_{(\id{ins}',\id{del}')}\ ;\ A_{(\id{ins},\id{del})}\\
	\Leftrightarrow\\
	&A_{((\id{ins}-\id{del}')\cup\id{ins}', \id{del}\cup\id{del}')}&=&A_{((\id{ins}'-\id{del})\cup\id{ins}, \id{del}'\cup\id{del})}\\
	\Leftrightarrow\\
	&(\id{ins}-\id{del}')\cup\id{ins}'&=&(\id{ins}'-\id{del})\cup\id{ins}\\
	\Leftrightarrow\\
	&(\id{ins}\cap\overline{\id{del}'})\cup\id{ins}'&=&(\id{ins}'\cap\overline{\id{del}})\cup\id{ins}\\
	\Leftrightarrow\\
	&(\id{ins}\cup\id{ins}')\cap(\overline{\id{del}'}\cup\id{ins}')&=&(\id{ins}\cup\id{ins}')\cap(\overline{\id{del}}\cup\id{ins})\\
	\Leftrightarrow\\
	&(\id{ins}\cup\id{ins}')-\overline{\overline{\id{del}'}\cup\id{ins}'}&=&(\id{ins}\cup\id{ins}')-\overline{\overline{\id{del}}\cup\id{ins}}\\
	\Leftrightarrow\\
	&\id{del}'\cap\overline{\id{ins}'}&=&\id{del}\cap\overline{\id{ins}}\\
	\Leftrightarrow\\
	&\id{del}'-\id{ins}'&=&\id{del}-\id{ins}
\end{array}\]
	For the purpose of making commutativity independent of its arguments,
	we must check the condition $\id{del}'-\id{ins}'=\id{del}-\id{ins}$.

\section{Isolation}
\label{sct:Isolation}
	If a user were alone,
	The moment a transaction starts, its user is isolated from other users.
	That moment is represented by a relation $\declare{beginTransaction}{Event}{Transaction}$,
	where $\pair{e}{t}\in\id{beginTransaction}$ means that event $e$ marks the start of transaction $t$.


\section{Ampersand}
\label{sct:Ampersand}
	In this section we explain the basics of Ampersand.
	The reader is expected to have sufficient background in relation algebra to understand the remainder of this paper.

	The core of an Ampersand script is a triple $\la\rules,\rels,\concepts\ra$,
	which consists of a set of rules $\rules$, relations $\rels$, and concepts $\concepts$.
	An Ampersand script is interpreted by the compiler as an information system.
	The rules constitute a theory in heterogeneous relation algebra.
	They constrain a body of data that resides in a database.

	The Ampersand compiler generates a database from relations in the script.
	A database application%
\footnote{Ampersand generates an application that consists of a relational database and interface components.
	Currently this application runs server-side on a PHP/MySQL platform and on a web-browser on the client-side.}
	assists users to keep rules satisfied throughout the lifetime of the database. It is also generated by Ampersand.

	A rule is a constraint in the form of an equality between two terms.
	Terms are built from relations.
	Ampersand interprets every relation as a finite set of pairs, which are stored in the database.
	The phase in which Ampersand takes a script and turns it into a database, is what we will refer to as \define{compile-time}.
	The phase in which a user interacts with the database, is what we will refer to as \define{run-time}.
	At run-time, Ampersand can decide which rules are satisfied by querying the database.
	The compiler generates all software needed to keep rules satisfied at run-time.
	If one of the rules is not satisfied as a result of data that has changed,
	that change is reverted (rolled back) to the original state.
	Either way, Ampersand ensures that all rules remain satisfied.
	
	Atoms are values that have no internal structure, meant to represent data elements in a database.
	From a business perspective, atoms are used to represent concrete items of the world,
	such as \atom{Peter}, \atom{1}, or \atom{the king of France}.
	By convention throughout the remainder of this paper, variables $a$, $b$, and $c$ are used to represent \emph{atoms}.
	The set of all atoms is called $\atoms$.
        Each atom is an instance of a \emph{concept}.

	Concepts (from set $\concepts$) are names we use to classify atoms in a meaningful way.
	For example, you might choose to classify \atom{Peter} as a person, and \atom{074238991} as a telephone number.
        We will use variables $A$, $B$, $C$, $D$ to represent concepts.
	The term $\ident{A}$ represents the \emph{identity relation} of concept $A$.
	The expression $a \in A$ means that atom $a$ is an \emph{instance} of concept $A$.
	The declaration of $A\isa B$ (pronounce: $A$ is a $B$)
	in an Ampersand script states that any instance of $A$ is an instance of $B$ as well.
	We call this {\em specialization}, but it is also known as {\em generalization} or {\em subtyping}.
	Specialization is needed to allow statements such as: ``An orange is a fruit that ....''.
	Specialization is not used in the remainder of this paper.

	Relations (from set $\rels$) are used in information systems to store data.
%	A \define{fact} is a statement that is true in a business context.
%	Facts are stored and kept as data in a computer.
	As data changes over time, so do the contents of these relations.
	In this paper relations are represented by variables $r$, $s$, and $d$.
	We represent the declaration of a relation $r$ by $\declare{nm}{A}{B}$,
	in which \id{nm} is a name and $A$ and $B$ are concepts.
	We call $A$ the source concept and $B$ the target concept of the relation.
	The pair $\pair{A}{B}$ is called the \emph{type} of the relation.
	The term $\fullt{A}{B}$ represents the \emph{universal relation} over concepts $A$ and $B$.

	The meaning of relations in Ampersand is defined by an interpretation function $\mathfrak{I}$.
	It maps each relation to a set of pairs.
	The type system of Ampersand guarantees that each pair in $r$ respects the type of $r$:
\begin{equation}
	\pair{a}{b}\in\ti{\declare{nm}{A}{B}} \Rightarrow\ a \in A \wedge b \in B \label{typing of relations}
\end{equation}
	The type system is strong, which means that every atom has a type.
	The type system is static, which means that type checking is entirely done at compile-time.

	Terms are used to combine relations using operators.
	The set of terms is called $\terms$.
\begin{definition}[terms]
\label{def:terms}
\item   The set of terms, $\terms$, is the smallest set that satisfies, for all $r,s \in \terms$, $d\in\rels$ and $A,B \in \concepts$: 
\begin{eqnarray}
	d&\in&\terms         \quad\quad\text{(every relation is a term)}\\
	(r \cup s)&\in&\terms\quad\quad\text{(union)}\\
	(r \cap s)&\in&\terms\quad\quad\text{(intersection)}\\
	(r-s)&\in&\terms     \quad\quad\text{(difference)}\label{def:difference}\\
	(r;s)&\in&\terms     \quad\quad\text{(composition)}\\
	(r\backslash s)&\in&\terms     \quad\quad\text{(right residual)}\\
	(r\slash s)&\in&\terms     \quad\quad\text{(left residual)}\\
	\flip r&\in&\terms   \quad\quad\text{(converse)}\\
	\cmpl{r}&\in&\terms   \quad\quad\text{(complement)}\\
%	\kleeneplus r&\in&\terms   \quad\quad\text{(Kleene closure)}\\
%	\kleenestar r&\in&\terms   \quad\quad\text{(Kleene closure)}\\
	\ident{A}&\in&\terms \quad\quad\text{(identity)}\\
	\fullt{A}{B}&\in&\terms \quad\quad\text{(full set)}
\end{eqnarray}
\end{definition}
	Throughout the remainder of this paper,	terms are represented by variables $r$, $s$, $d$, and $t$.
	The \define{type} of a term $r$ is a pair of concepts, given by $\tf{r}$.
	$\mathfrak{T}$ is a partial function that maps terms to types.
	If a term has one type, it is called \define{type correct}.
	The Ampersand compiler requires all terms to be type correct.
	If no type can be assigned, the compiler gives an error message.
	If multiple types are assignable to a term, the ambiguity is signalled by the compiler.
	In that case it will also emit an error message and prompt the programmer to disambiguate the term by adding type information.
	In practice, the derivation of types can disambiguate most terms.
	So, the programmer experiences some freedom to denote distinct relations by the same name without the obligation to specify their types at all occurrences.
	The compiler does not generate any code if there is a term that is not type correct.
	The type function and its restrictions are discussed in~\cite{Joosten2015}
	and have no consequences for the remainder of this paper.

 	The meaning of terms in Ampersand is given by an interpretation function $\mathfrak{I}$.
	Let $A$ and $B$ be finite sets of atoms, then $\mathfrak{I}$ maps each term to the set of pairs for which that term stands.
\begin{definition}[interpretation of terms]
\label{interpretation of terms}
\item   For every $A,B\in\concepts$ and $r,s\in\terms$
\begin{eqnarray}
	\ti{r}		 &=&\{\pair{a}{b}|\ a\ r\ b\}	\\
	\ti{r \cup s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{or }\ \pair{a}{b}\in\ti{s}\}	\\
	\ti{r \cap s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{a}{b}\in\ti{s}\}	\\
	\ti{r-s}	 &=&\{\pair{a}{b}|\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{a}{b}\notin\ti{s}\}	\\
	\ti{r;s}	 &=&\{\pair{a}{c}|\ \text{for some}\ b,\ \pair{a}{b}\in\ti{r}\ \text{and}\ \pair{b}{c}\in\ti{s}\}	\\
	\ti{r\backslash s}	 &=&\{\pair{b}{c}|\ \text{for all}\ a,\ \pair{a}{b}\in\ti{r}\ \text{implies}\ \pair{a}{c}\in\ti{s}\}	\\
	\ti{r\slash s}	 &=&\{\pair{a}{b}|\ \text{for all}\ c,\ \pair{b}{c}\in\ti{s}\ \text{implies}\ \pair{a}{c}\in\ti{r}\}	\\
	\ti{\flip{r}}	 &=&\{\pair{b}{a}|\ \pair{a}{b}\in\ti{r}    \}	\\
	\ti{\cmpl{\declare{r}{A}{B}}}	 &=&\fullt{A}{B}-r	\\
%	\ti{\kleeneplus{r}}	 &=&\text{the smallest set that satisfies }\kleeneplus{r}=r\ \cup\ \kleeneplus{r};\kleeneplus{r}	\\
%	\ti{\kleenestar{\declare{r}{A}{A}}}	 &=&\ident{A}\cup\kleeneplus{r}\\
	\ti{\ident{A}} 	 &=&\{\pair{a}{a}|\ a\in A\}	\\
	\ti{\fullt{A}{B}}&=&\{\pair{a}{b}|\ a\in A, b\in B\}
\end{eqnarray}
\end{definition}
%	The Kleene closure operators (postfix $\kleeneplus{\ }$ and $\kleenestar{\ }$) have been implemented partially in the current Ampersand implementation.

	A \define{rule} is a pair of terms $r,s\in\terms$ with $\tf{r}=\tf{s}$, which is syntactically recognizable as a rule.
\[\text{RULE}\ r = s\]
	This means \(\ti{r} = \ti{s}\). In practice, many rules are written as:
\[\text{RULE}\ r\subs s\]
	This is a shorthand for 
\[\text{RULE}\ r\cap s = r\]
	We have enhanced the type function $\mathfrak{T}$ and the interpretation function $\mathfrak{I}$ to cover rules as well.
	If $\tf{r}=\tf{s}$ and $\tf{s}=\pair{A}{B}$:
\begin{eqnarray}
	\tf{\text{RULE}\ r = s}   &=&\pair{A}{B}\\
	\tf{\text{RULE}\ r\subs s}&=&\pair{A}{B}\\
 resolved this \end{eqnarray}

	We call a rule $\text{RULE}\ r = s$ \define{satisfied} when its interpretation equals $\ti{\fullt{A}{B}}$.
	As the population of relations used in $r$ changes with time, the satisfaction of the rule changes accordingly.
	A software developer, who conceives these rules, must consider how to keep each one of them satisfied.
	We call a rule \define{violated} if it is not satisfied.
	The set $\ti{(s-r)\cup(r-s)}$ is called the \emph{violation set} of \(\text{RULE}\ r = s\).
	To \define{resolve} violations means to change the contents of relations such that the rule is satisfied%
\footnote{To \define{restore invariance} is sometimes used as a synonym to resolving violations.
	To \define{preserve invariance} is sometimes used as a synonym to keeping a rule satisfied.}.
	Consequently, such a rule may also be referred to as \define{invariant}.
	Some ways of resolving violations are presented in section~\ref{sct:Code Fragments}.
	Each pair in the violation set of a rule is called a violation of that rule.

	The software developer must define how to resolve violations when they occur.
	She does so by inserting and/or deleting pairs in appropriately chosen relations.
	Whatever choice she makes, she must ensure that her code yields data that satisfies the rules.
	When we say: ``rule $r$ specifies this action'' we mean that satisfaction of rule $r$ is a postcondition of any action specified by rule $r$. 

\section{Software for Legal Reasoning}
\label{sct:Conceptual analysis}

% \begin{figure}[htb]
% \begin{center}
%   \includegraphics[angle=90,scale=.357]{LogicalDataModel.pdf}
% \end{center}
% \caption{Conceptual data model (from section~\ref{sct:Conceptual analysis})}
% \label{fig:conceptual model}
% \end{figure}

\section{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{doc}


\end{document}
